{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dynamic Selection Hybrid Model for Advancing Thyroid Care With BOOST Balancing Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================================\n",
    "\n",
    "#### Overview of the code:\n",
    "\n",
    "1. Data collection\n",
    "\n",
    "2. Analyze the data\n",
    "\n",
    "3. Daata preprocessing\n",
    "    * Drop unwanted columns and duplicates\n",
    "    \n",
    "    * Handle Null values\n",
    "        * Drop if it contains more than 50% null values\n",
    "        * Fill or replace null values with mean, median or mode.\n",
    "\n",
    "    * Implement Labelencoding to convert object type columns into numeric columns.\n",
    "\n",
    "    * Implement BOOST balancing method to balance the data.\n",
    "\n",
    "        BOOST :=>\n",
    "        - BS(Boosting with Sample Weighting), \n",
    "        - SMOTE (Synthetic Minority Over-sampling Technique),\n",
    "        - Tomek Links (TL)\n",
    "        \n",
    "            implementation:\n",
    "            - Split dataset into training and test sets\n",
    "            - Step 1: Apply SMOTE for oversampling minority class\n",
    "            - Step 2: Apply Tomek Links to remove noisy samples\n",
    "            - Step 3: Apply Boosting Stage (BS) => Model implementation\n",
    "\n",
    "4. Build a Dynamic Selection Hybrid Model\n",
    "\n",
    "    implementation:\n",
    "    \n",
    "    * Define the classifiers\n",
    "    * Step 1: Train all classifiers and compute Permutation Feature Importance (PFI)\n",
    "    * Step 2: Select Half-Most Effective Classifiers (HEC) based on PFI\n",
    "    * Step 3: Define the ensemble methods using the selected classifiers\n",
    "    * Step 4: Train each ensemble method and evaluate accuracy\n",
    "    * Step 5: Select Most Efficient Ensemble Method (EEM)\n",
    "    * Build final model\n",
    "\n",
    "5. Save the model for deploy into web application frontend\n",
    "\n",
    "6. sample predictions\n",
    "\n",
    "======================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.10.14)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset and store it a a dataframe\n",
    "df = pd.read_csv(\"DATASET/thyroidDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows, columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Object columns\n",
    "num_object_columns = len(df.select_dtypes(include='object').columns)\n",
    "print(f\"Number of object columns: {num_object_columns}\")\n",
    "\n",
    "# Count of numeric columns\n",
    "num_numeric_columns = len(df.select_dtypes(include='number').columns)\n",
    "print(f\"Number of numeric columns: {num_numeric_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of null rows\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total missing values and percentage of missing values\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "train_total = df.isnull().sum()\n",
    "train_percent = (train_total / df.shape[0]) * 100\n",
    "\n",
    "# Create a DataFrame to hold this information\n",
    "data_missing = pd.DataFrame({\n",
    "    'Total nulls': train_total,\n",
    "    'Percentage': train_percent\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the 'Total nulls' column in descending order\n",
    "data_missing_sorted = data_missing.sort_values(by='Total nulls', ascending=False)\n",
    "\n",
    "# Convert the DataFrame to HTML\n",
    "html = data_missing_sorted.to_html()\n",
    "\n",
    "# Display the HTML as a scrollable element\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"height:300px; overflow-y:scroll; border:1px solid black; padding:10px;\">\n",
    "    {html}\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot for target column\n",
    "object_columns = [\"target\"]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(object_columns):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.countplot(x=df[feature], data=df)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Description;\n",
    "\n",
    "\n",
    "| S.No. | Feature Name           | Description                                                 | Data Type |\n",
    "|-------|------------------------|-------------------------------------------------------------|-----------|\n",
    "| 1     | age                    | age of the patient                                          | int       |\n",
    "| 2     | sex                    | sex patient identifies                                      | str       |\n",
    "| 3     | on_thyroxine           | whether patient is on thyroxine                             | bool      |\n",
    "| 5     | on_antithyroid_meds    | Whether patient is on antithyroid meds                      | bool      |\n",
    "| 6     | sick                   | Whether patient is sick                                     | bool      |\n",
    "| 7     | pregnant               | Whether patient is pregnant                                 | bool      |\n",
    "| 8     | thyroid_surgery        | Whether patient has undergone thyroid surgery               | bool      |\n",
    "| 9     | I131_treatment         | Whether patient is undergoing I131 treatment                | bool      |\n",
    "| 10    | query_hypothyroid      | Whether patient believes they have hypothyroid              | bool      |\n",
    "| 11    | query_hyperthyroid     | Whether patient believes they have hyperthyroid             | bool      |\n",
    "| 12    | lithium                | Whether patient use lithium. This will cause goiter and hypothyroidism                               | bool      |\n",
    "| 13    | goitre                 | Whether patient has goitre. It is an abnormal enlargement of the thyroid gland | bool      |\n",
    "| 14    | tumor                  | Whether patient has tumor                                   | bool      |\n",
    "| 15    | hypopituitary          | Whether patient has a hypopituitary gland                   | float     |\n",
    "| 16    | psych                  | Whether patient has psych issues                            | bool      |\n",
    "| 17    | TSH_measured           | Whether TSH was measured in the blood                       | bool      |\n",
    "| 18    | TSH                    | TSH level in blood from lab work                            | float     |\n",
    "| 19    | T3_measured            | Whether T3 was measured in the blood                        | bool      |\n",
    "| 20    | T3                     | T3 level in blood from lab work                             | float     |\n",
    "| 21    | TT4_measured           | Whether TT4 was measured in the blood                       | bool      |\n",
    "| 22    | TT4                    | TT4 level in blood from lab work                            | float     |\n",
    "| 23    | T4U_measured           | Whether T4U was measured in the blood                       | bool      |\n",
    "| 24    | T4U                    | T4U level in blood from lab work                            | float     |\n",
    "| 25    | FTI_measured           | Whether FTI was measured in the blood                       | bool      |\n",
    "| 26    | FTI                    | FTI level in blood from lab work                            | float     |\n",
    "| 27    | TBG_measured           | Whether TBG was measured in the blood                       | bool      |\n",
    "| 28    | TBG                    | TBG level in blood from lab work                            | float     |\n",
    "| 29    | referral_source        | Referral source                                             | str       |\n",
    "| 30    | patient_id             | Unique ID of the patient                                    | str       |\n",
    "\n",
    "\n",
    "Target:\n",
    " \t* target - hyperthyroidism medical diagnosis (str)\n",
    "\n",
    "we are going to define user have Hypothyroid or Hyperthyroid and their conditions (Totally 8 classifications).\n",
    "\n",
    "            1. hyperthyroid conditions:\n",
    "                1) A   Subclinical (initial)\n",
    "                2) B   T3 toxic\n",
    "                3) C   toxic goitre\n",
    "                4) D   secondary toxic\n",
    "\n",
    "            2. hypothyroid conditions:\n",
    "                5) E   Subclinical (initial)\n",
    "                6) F   primary hypothyroid\n",
    "                7) G   compensated hypothyroid\n",
    "                8) H   secondary hypothyroid\n",
    "\n",
    "\n",
    "In our dataset;\n",
    "* There are 920 rows and 31 columns,\n",
    "* Number of object columns: 23,\n",
    "* Number of numeric columns: 8,\n",
    "* There one duplicate,\n",
    "* Null values:\n",
    "\n",
    "    | Column | Total Nulls | Percentage |\n",
    "    |--------|-------------|------------|\n",
    "    | TBG    | 912         | 99.130435  |\n",
    "    | T3     | 200         | 21.739130  |\n",
    "    | T4U    | 55          | 5.978261   |\n",
    "    | FTI    | 54          | 5.869565   |\n",
    "    | TSH    | 47          | 5.108696   |\n",
    "    | sex    | 42          | 4.565217   |\n",
    "    | TT4    | 9           | 0.978261   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For preprocess the data we are going to;\n",
    "1. Drop unwanted columns (patient_id) and duplicates\n",
    "2. Handle Null values:\n",
    "    * Drop TBG\tcolumn. Beacause it contains 912 null values which means 99% null values.\n",
    "    * Fill or replace T3, TSH, T4U, FTI, TT4, sex columns with mean, median or mode.\n",
    "3. Implement BOOST balancing method to balance the data.\n",
    "4. Implement Labelencoding to convert object type columns into numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop unwanted columns\n",
    "patient_id column is no need. So we can drop that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"patient_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handle null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop TBG column. Beacause it contains 912 null values which means 99% null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"TBG\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill or replace T3, TSH, T4U, FTI, TT4, sex columns with mean, median or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_nulls_with_random(df):\n",
    "    # Identify numerical columns\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Loop through each numerical column\n",
    "    for col in numerical_columns:\n",
    "        # Check for null values\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            # Get lower and upper bounds (min and max of non-null values)\n",
    "            lower_bound = df[col].min()\n",
    "            upper_bound = df[col].max()\n",
    "            \n",
    "            # Generate random numbers to fill the null values\n",
    "            # We only generate numbers for the number of NaNs in the column\n",
    "            random_values = np.random.uniform(lower_bound, upper_bound, df[col].isnull().sum())\n",
    "            \n",
    "            # Fill NaN values with generated random numbers\n",
    "            df.loc[df[col].isnull(), col] = random_values\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_filled = fill_nulls_with_random(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for object (categorical) columns\n",
    "object_df = df.select_dtypes(include='object')\n",
    "\n",
    "# Function to fill null values with random values from unique values of the column\n",
    "def fill_na_with_random_choice(column):\n",
    "    if column.isnull().any():  # Check if there are NaNs in the column\n",
    "        unique_values = column.dropna().unique()  # Get unique values, excluding NaNs\n",
    "        if len(unique_values) > 0:\n",
    "            # Generate random choices for each NaN\n",
    "            random_choices = np.random.choice(unique_values, size=column.isnull().sum())\n",
    "            column.fillna(pd.Series(random_choices, index=column[column.isnull()].index), inplace=True)\n",
    "\n",
    "# Apply the function to each object column\n",
    "for col in object_df.columns:\n",
    "    fill_na_with_random_choice(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for object (categorical) columns\n",
    "object_df = df.select_dtypes(include='object')\n",
    "\n",
    "# Identify object columns with fewer than 2 unique values\n",
    "columns_with_fewer_than_two_classes = [\n",
    "    col for col in object_df.columns\n",
    "    if object_df[col].nunique() < 2\n",
    "]\n",
    "\n",
    "print(\"Object columns with fewer than 2 unique values:\\n\", columns_with_fewer_than_two_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the object data.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Store original column names\n",
    "original_columns = df.select_dtypes(include='object').columns\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "# Apply LabelEncoder to each categorical variable\n",
    "for col in original_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# Print the mapping between original categories and numerical labels\n",
    "for col, encoder in label_encoders.items():\n",
    "    print(f\"Mapping for column '{col}':\")\n",
    "    for label, category in enumerate(encoder.classes_):\n",
    "        print(f\"Label {label}: {category}\")\n",
    "    print(\"===============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(\"Correlation Matrix:\\n\", correlation_matrix)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"target\"], axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Initialize SelectKBest with the desired scoring function and k=10 (10 best features)\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Fit the selector\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get the feature scores for each feature\n",
    "scores = selector.scores_\n",
    "p_values = selector.pvalues_\n",
    "\n",
    "# Create a DataFrame with feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Score': scores,\n",
    "    'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by Score in descending order\n",
    "feature_scores_sorted = feature_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# top 10 features\n",
    "top_10_features = feature_scores_sorted.head(10)\n",
    "\n",
    "# Retrieve the column names of the top 10 features\n",
    "top_10_feature_names = top_10_features['Feature'].tolist()\n",
    "\n",
    "# Filter the original DataFrame to include only the top 10 features\n",
    "df_top_10 = df[top_10_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view thedataframe\n",
    "df_top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BOO-ST balance method to balance the imbalace data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOST :=>\n",
    "* BS(Boosting with Sample Weighting), \n",
    "* SMOTE (Synthetic Minority Over-sampling Technique),\n",
    "* Tomek Links (TL)\n",
    "\n",
    "- First, apply SMOTE to oversample the minority class.\n",
    "- Then, apply Tomek Links to remove noisy data points.\n",
    "- Finally, train the boosting model on the resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data Into X and y (X has all features and y has target variable)\n",
    "X = df_top_10\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Apply SMOTE for oversampling minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=5)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Apply Tomek Links to remove noisy samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek = TomekLinks()\n",
    "X_resampled, y_resampled = tomek.fit_resample(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resampled data into DataFrames\n",
    "X_train_res_df = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "y_train_res_df = pd.DataFrame(y_resampled, columns=[y_train.name])  # y_train.name preserves the original target column name\n",
    "\n",
    "# Combine the features and target into one DataFrame (preprocessed data)\n",
    "df = pd.concat([X_train_res_df, y_train_res_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the preprocessed data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "print(\"Duplicates before drop:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicates before drop:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null counts\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot for target column to check the whether the data balanced or not\n",
    "object_columns = [\"target\"]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(object_columns):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.countplot(x=df[feature], data=df)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessed data is a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about preprocessde data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about preprocessed data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the data we have;\n",
    "* No duplicates,\n",
    "* No null values,\n",
    "* No object columns (Converted all object into numeric type using label encoding)\n",
    "* Balanced dataset\n",
    "\n",
    "Now we can go for Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Apply Boosting Stage (BS)\n",
    "\n",
    "Now our balanced dataset is ready. Now we can train the boosting model on the resampled dataset.\n",
    "For that we are going to implement Dynamic Selection Hybrid Model \n",
    "\n",
    "Dynamic Selection Hybrid Model \n",
    "\n",
    "* Define the classifiers\n",
    "* Step 1: Train all classifiers and compute Permutation Feature Importance (PFI)\n",
    "* Step 2: Select Half-Most Effective Classifiers (HEC) based on PFI\n",
    "* Step 3: Define the ensemble methods using the selected classifiers\n",
    "* Step 4: Train each ensemble method and evaluate accuracy\n",
    "* Step 5: Select Most Efficient Ensemble Method (EEM)\n",
    "* Build final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save preprocessed data as a csv file for web application use\n",
    "# df.to_csv(\"DATASET/Final_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df.drop(\"target\", axis=1) # Features\n",
    "y = df[\"target\"] # Target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing features\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing features\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing target\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Algorithm Impementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" Permutation Feature Importance (PFI) is a method used to evaluate the importance of features in a machine learning model. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train all classifiers and compute Permutation Feature Importance (PFI)\n",
    "pfi_results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate testing accuracy\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Testing Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(report)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix using seaborn heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=clf.classes_, yticklabels=clf.classes_)    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    pfi = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42)\n",
    "    pfi_results[name] = np.mean(pfi.importances_mean)\n",
    "    print(f\"Classifier: {name}, PFI Score: {pfi_results[name]}\")\n",
    "    print(\"=============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Select Half-Most Effective Classifiers (HEC) based on PFI\n",
    "sorted_classifiers = sorted(pfi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "hec_classifiers = [name for name, _ in sorted_classifiers[:len(sorted_classifiers) // 2]]\n",
    "print(f\"Selected HEC Classifiers: {hec_classifiers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the ensemble methods using the selected classifiers\n",
    "estimators = [(name, classifiers[name]) for name in hec_classifiers]\n",
    "\n",
    "# Define ensemble methods\n",
    "boosting = AdaBoostClassifier(estimator=estimators[0][1])\n",
    "bagging = BaggingClassifier(estimator=estimators[0][1])\n",
    "voting = VotingClassifier(estimators=estimators, voting='soft')\n",
    "stacking = StackingClassifier(estimators=estimators)\n",
    "\n",
    "ensemble_methods = {\n",
    "    'Boosting': boosting,\n",
    "    'Bagging': bagging,\n",
    "    'Voting': voting,\n",
    "    'Stacking': stacking\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train each ensemble method and evaluate accuracy\n",
    "accuracy_results = {}\n",
    "for name, ensemble in ensemble_methods.items():\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    y_pred = ensemble.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_results[name] = accuracy\n",
    "    print(f\"Ensemble Method: {name}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Select Most Efficient Ensemble Method (EEM)\n",
    "best_ensemble = max(accuracy_results, key=accuracy_results.get)\n",
    "print(f\"Most Efficient Ensemble Method: {best_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = ensemble_methods[best_ensemble]\n",
    "\n",
    "# Make predictions with the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate training accuracy\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy for {best_ensemble}: {train_accuracy:.4f}\")\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Testing Accuracy for {best_ensemble}: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f\"\\nClassification Report for {best_ensemble}:\")\n",
    "print(report)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "plt.title(f'Confusion Matrix for {best_ensemble}')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_model, 'MODELS/best_ensemble_model.pkl')\n",
    "print(\"Best model saved as 'best_ensemble_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "model = joblib.load('MODELS/best_ensemble_model.pkl')\n",
    "\n",
    "# Define class mappings\n",
    "condition_class = {\n",
    "    0: \"Subclinical (initial level)\", \n",
    "    1: \"T3 toxic\", \n",
    "    2: \"toxic goitre\", \n",
    "    3: \"secondary toxic\", \n",
    "    4: \"Subclinical (initial level)\",\n",
    "    5: \"primary hypothyroid\",\n",
    "    6: \"compensated hypothyroid\",\n",
    "    7: \"secondary hypothyroid\",\n",
    "}\n",
    "\n",
    "disorder_class = {\n",
    "    0: \"hyperthyroid\", \n",
    "    1: \"hyperthyroid\", \n",
    "    2: \"hyperthyroid\", \n",
    "    3: \"hyperthyroid\", \n",
    "    4: \"hypothyroid\",\n",
    "    5: \"hypothyroid\",\n",
    "    6: \"hypothyroid\",\n",
    "    7: \"hypothyroid\",\n",
    "}\n",
    "\n",
    "# Prediction Function\n",
    "def prediction_func(input_features):\n",
    "    input_array = np.array([input_features])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_array)\n",
    "    \n",
    "    # Convert prediction to class label\n",
    "    predicted_class = prediction[0]\n",
    "\n",
    "    # Map the class label to disorder and condition\n",
    "    predicted_disorder = disorder_class[predicted_class]\n",
    "    predicted_condition = condition_class[predicted_class]\n",
    "    \n",
    "    print(f\"Predicted Disorder: {predicted_disorder}\")\n",
    "    print(f\"Predicted Condition: {predicted_condition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hyperthyroid (Subclinical)\n",
    "prediction_func([160.000000, 0, 204.000000, 1, 0, 0.030000, 0, 0.780000, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hyperthyroid (T3 toxic)\n",
    "prediction_func([140.489084, 0, 117.644102, 1, 0, 0.030775, 0, 1.192227, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hyperthyroid (toxic goitre)\n",
    "prediction_func([117.830119, 1, 106.245178, 1, 0, 0.865798, 0, 1.105849, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hyperthyroid (secondary toxic)\n",
    "prediction_func([131.975559, 0, 487.814102, 1, 0, 7.953860, 0, 0.294808, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hypothyroid (Subclinical)\n",
    "prediction_func([16.000000, 0, 15.000000, 0, 0, 298.456436, 0, 1.100000, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hypothyroid (primary hypothyroid)\n",
    "prediction_func([3.900000, 0, 5.000000, 1, 0, 70.000000, 0, 0.830000, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hypothyroid (compensated hypothyroid)\n",
    "prediction_func([78.000000, 0, 85.000000, 1, 0, 23.000000, 0, 0.920000, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input for hypothyroid (secondary hypothyroid)\n",
    "prediction_func([47.263362, 0, 54.475693, 1, 0, 5.507901, 0, 0.874653, 0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
